{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "940352dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "# nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d98d8ff9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#read the data into pandas dataframes.\n",
    "admissionsDf = pd.read_csv(\"Data/ADMISSIONS.csv\")\n",
    "diagnosesDf = pd.read_csv(\"Data/DIAGNOSES_ICD.csv\")\n",
    "eventsDf = pd.read_csv(\"Data/NOTEEVENTS.csv\", dtype={\"CHARTTIME\":\"string\", \"STORETIME\":\"string\"})\n",
    "patientsDf = pd.read_csv(\"Data/PATIENTS.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92773a5",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "- Drop records for patients under 15 years of age - DONE!\n",
    "- Use patient's first admission to ICU only - DONE!\n",
    "- Segregate and deduplicate patient records - DONE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b8ea3c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort admissions dataframe by subject id and date and drop duplicate subject id so that only patient record for first visit is retained\n",
    "admissionsDf.sort_values([\"SUBJECT_ID\", \"ADMITTIME\"], ascending=[True, True], inplace=True)\n",
    "admissionsDf.drop_duplicates(subset=[\"SUBJECT_ID\"], inplace=True)\n",
    "\n",
    "dobDf = patientsDf[[\"SUBJECT_ID\", \"DOB\"]] #create a dataframe of patient id and corresponding date of birth\n",
    "admissionsDf1 = pd.merge(admissionsDf, dobDf, how=\"left\", on=\"SUBJECT_ID\") #merge the date of births to the admissions dataframe and reassign the admissions dataframe\n",
    "admissionsDf1[\"ADMITTIME\"] = pd.to_datetime(admissionsDf1[\"ADMITTIME\"]) #convert admit time to datetime\n",
    "admissionsDf1[\"DOB\"] = pd.to_datetime(admissionsDf1[\"DOB\"]) #convert DOB to datetime \n",
    "\n",
    "admissionsDf1['AGE'] = (admissionsDf1[\"ADMITTIME\"].values - admissionsDf1[\"DOB\"].values) / np.timedelta64(1,\"D\") // 365 #calculate the age of each patient at the time of admission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a091b60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the patient id for all patients < 15 years old\n",
    "under15 = admissionsDf1.loc[abs(admissionsDf1[\"AGE\"]) < 15.0]\n",
    "under15Patients = list(under15[\"SUBJECT_ID\"]) #7,875 patients under 15 years\n",
    "\n",
    "#note: conflicting information in the paper about age filter (page 1155 indicates >= 15 years while page 1156 indicates\n",
    "# > 15 years). We are using >= 15 years as candidate patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "46d18129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop under15 patients from admissions, diagnoses, events and patients\n",
    "admissionsFiltered = admissionsDf1[~admissionsDf1.SUBJECT_ID.isin(under15Patients)]\n",
    "diagnosesFiltered = diagnosesDf[~diagnosesDf.SUBJECT_ID.isin(under15Patients)]\n",
    "eventsFiltered = eventsDf[~eventsDf.SUBJECT_ID.isin(under15Patients)]\n",
    "patientsFiltered = patientsDf[~patientsDf.SUBJECT_ID.isin(under15Patients)] #38,645 adult (>=15 years) patients\n",
    "\n",
    "#Note: the FarSight paper incorrectly states there are 7,704 distinct patients (page 1155)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7c5cf7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eventsNoError = eventsFiltered.loc[eventsFiltered.ISERROR != 1] #drop events with known errors. i.e. ISERROR = 1\n",
    "eventsNoDuplicate = eventsNoError.drop_duplicates() #drop duplicate events from the filtered dataframe\n",
    "patientsNoErrors = sorted(list(eventsFiltered.SUBJECT_ID.unique())) #create list of patients from filtered events which have no errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "823d6f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select patient id and hospital admission code from admissions dataframe and use to merge left with diagnoses and events\n",
    "patientHadmCode = admissionsFiltered[[\"SUBJECT_ID\", \"HADM_ID\"]]\n",
    "diagnosesFiltered1 = pd.merge(patientHadmCode, diagnosesFiltered, how=\"left\", on=[\"SUBJECT_ID\", \"HADM_ID\"])\n",
    "eventsFiltered1 = pd.merge(patientHadmCode, eventsNoDuplicate, how=\"left\", on=[\"SUBJECT_ID\", \"HADM_ID\"])\n",
    "eventsFiltered1[[\"TEXT\"]] = eventsFiltered1[[\"TEXT\"]].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "073720fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the final dataframes for further analysis\n",
    "admissions = admissionsFiltered[admissionsFiltered.SUBJECT_ID.isin(patientsNoErrors)] #final admissions dataframe to be used for further analysis\n",
    "diagnoses = diagnosesFiltered1[diagnosesFiltered1.SUBJECT_ID.isin(patientsNoErrors)].sort_values(by=\"SUBJECT_ID\") #final diagnoses dataframe to be used for further analysis\n",
    "patients = patientsFiltered[patientsFiltered.SUBJECT_ID.isin(patientsNoErrors)] #final patients dataframe to be used for further analysis\n",
    "events = eventsFiltered1[eventsFiltered1.SUBJECT_ID.isin(patientsNoErrors)].sort_values(by=[\"SUBJECT_ID\", \"CHARTDATE\"]) #final events dataframe to be used for further analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458e72f0",
   "metadata": {},
   "source": [
    "### Data Aggregation (using FarSight approach)\n",
    "- Concatenate the chronological notes for each patient - DONE!\n",
    "- Create a set of all ICD-9 codes for each patient - DONE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6724677f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of sets where each set contains all icd9 codes for a single patient\n",
    "diag_grouped = diagnoses[[\"SUBJECT_ID\", \"ICD9_CODE\"]].groupby(by=\"SUBJECT_ID\")[\"ICD9_CODE\"].apply(set).reset_index(name=\"Code_groups\")\n",
    "icd9_grouped = list(diag_grouped[\"Code_groups\"]) #list of ICD9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8d20d187",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of strings where each string is the concatenated nursing texts for a single patient. \n",
    "events_grouped = events[[\"SUBJECT_ID\", \"TEXT\"]].groupby(by=\"SUBJECT_ID\")[\"TEXT\"].apply(list).reset_index(name=\"text_groups\")\n",
    "notes_grouped = list(events_grouped[\"text_groups\"]) #list of grouped notes\n",
    "notes_list_all = [\" \".join(i) for i in notes_grouped] #list of concatenated notes for each patient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ecc86e",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "- Remove multiple spaces and special characters - DONE!\n",
    "- Tokenization (using NLTK) - DONE!\n",
    "- Stopword removal from generated tokens (using the NLTK English stopword corpus) - DONE!\n",
    "- Remove punctuation marks except hyphens and slashes - DONE!\n",
    "- Remove references to images - DONE!\n",
    "- Perform character case folding - DONE!\n",
    "- Perform medical concept normalization through disambiguation of abbreviations (into long form) using CARD - FOLLOW UP!\n",
    "- Perform suffix stripping through stemming - DONE!\n",
    "- Convert stripped tokens into their respective base forms by lemmatization - DONE!\n",
    "- Discard tokens appearing in less than 10 nursing notes - DONE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4759a2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! use a subset of the data from faster testing. comment out this line for final submission\n",
    "notes_list = notes_list_all[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b2110700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to remove multiple spaces, special characters and newlines\n",
    "def clean_notes(note):\n",
    "    a = re.sub(\"[^a-zA-Z0-9-/]+\", \" \", str(note)) #remove special characters\n",
    "    b = re.sub(\"\\n\", \"\", str(a)) #remove newlines\n",
    "    c = re.sub(\" +\", \" \", str(b)) #remove extra spaces\n",
    "    return c\n",
    "\n",
    "#map the clean notes funciton to the notes_list\n",
    "notes_list = list(map(clean_notes, notes_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a6d2d0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the nursing notes text\n",
    "tokens = [] #empty tokens list\n",
    "stops = set(stopwords.words('english')) #English stopwords corpus\n",
    "pStem = PorterStemmer() #instance of stemmer for suffix stripping. Less aggressive stemmer\n",
    "lStem = LancasterStemmer() #instance of stemmer for suffix stripping. Use either this or pStem\n",
    "wLemma = WordNetLemmatizer() #instance of lemmatizer\n",
    "img1=\".jpeg\"; img2=\".jpg\"; img3=\".png\"; img4=\".tiff\"; img5=\".bmp\" #image references for lookup\n",
    "\n",
    "for idx,txt in enumerate(notes_list):\n",
    "    a = nltk.word_tokenize(txt) #tokenize each nursing note\n",
    "    tokens.append([wLemma.lemmatize(pStem.stem(word.casefold()), pos=\"v\") for word in a if not word in stops \\\n",
    "                   if not img1 in word if not img2 in word if not img3 in word if not img4 in word if not img5 in word])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c11d4de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eliminate tokens appearing in less than 10 nursing texts\n",
    "token_counts = Counter([token for l in tokens for token in l])\n",
    "under_10_tokens = [t for (t,c) in token_counts.items() if c < 10]\n",
    "\n",
    "for idx, note in enumerate(tokens):\n",
    "    tokens[idx] = [token for token in note if not token in under_10_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dd26af",
   "metadata": {},
   "source": [
    "### NOTE!!!\n",
    "At this point we have:\n",
    "- patientsNoErrors: which is a list of patients ids (age >= 15) arranged in ascending order\n",
    "- icd9_grouped: which is a list of sets of icd9 codes where each set contains the icd9 codes for an individual patient. Each set of icd9 codes is for the patient at the corresponding index in patientsNoErrors. \n",
    "- tokens: which is a list of of lists where each list contains the normalized tokens for an individual patient where these tokens are generated using the concatenated nursing notes for said patient. Each list of tokens is for the patient at the corresponding index in patientsNoErrors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e71656a",
   "metadata": {},
   "source": [
    "### Clinical feature modeling - VECTOR SPACE MODELING OF CLINICAL NOTES\n",
    "- Obtain the Doc2Vec style features from the normalized nursing note tokens. Utilize the implementation in the Python Gensim package, with an embedding size of 500 (trained for 25 epochs), determined empirically using grid-search as per the original Farsight paper. - DONE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "20fc1476",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of TaggedDocument and train Doc2Vec model \n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(tokens)] #create list of TaggedDocument\n",
    "d2vModel = Doc2Vec(vector_size=500, epochs=25, dm=1) #create model\n",
    "d2vModel.build_vocab(documents) #build the vocab\n",
    "d2vModel.train(documents, total_examples=d2vModel.corpus_count, epochs=d2vModel.epochs) #train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "23cb3218",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#map the tokens to Doc2Vec style features using the trained model\n",
    "d2v_tokens = [d2vModel.infer_vector(i) for i in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c16d791",
   "metadata": {},
   "source": [
    "### Clinical feature modeling - TOPIC MODELING OF CLINICAL NOTES\n",
    "- Obtain the Doc2Vec style features from the normalized nursing note tokens. Utilize the implementation in the Python Gensim package, with an embedding size of 500 (trained for 25 epochs), determined empirically using grid-search as per the original Farsight paper. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
