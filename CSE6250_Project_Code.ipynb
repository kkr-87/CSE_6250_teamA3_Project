{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "940352dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "# nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models.nmf import Nmf\n",
    "# from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d98d8ff9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#read the data into pandas dataframes.\n",
    "admissionsDf = pd.read_csv(\"Data/ADMISSIONS.csv\")\n",
    "diagnosesDf = pd.read_csv(\"Data/DIAGNOSES_ICD.csv\")\n",
    "eventsDf = pd.read_csv(\"Data/NOTEEVENTS.csv\", dtype={\"CHARTTIME\":\"string\", \"STORETIME\":\"string\"})\n",
    "patientsDf = pd.read_csv(\"Data/PATIENTS.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92773a5",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "- Drop records for patients under 15 years of age - DONE!\n",
    "- Use patient's first admission to ICU only - DONE!\n",
    "- Segregate and deduplicate patient records - DONE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8ea3c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort admissions dataframe by subject id and date and drop duplicate subject id so that only patient record for first visit is retained\n",
    "admissionsDf.sort_values([\"SUBJECT_ID\", \"ADMITTIME\"], ascending=[True, True], inplace=True)\n",
    "admissionsDf.drop_duplicates(subset=[\"SUBJECT_ID\"], inplace=True)\n",
    "\n",
    "dobDf = patientsDf[[\"SUBJECT_ID\", \"DOB\"]] #create a dataframe of patient id and corresponding date of birth\n",
    "admissionsDf1 = pd.merge(admissionsDf, dobDf, how=\"left\", on=\"SUBJECT_ID\") #merge the date of births to the admissions dataframe and reassign the admissions dataframe\n",
    "admissionsDf1[\"ADMITTIME\"] = pd.to_datetime(admissionsDf1[\"ADMITTIME\"]) #convert admit time to datetime\n",
    "admissionsDf1[\"DOB\"] = pd.to_datetime(admissionsDf1[\"DOB\"]) #convert DOB to datetime \n",
    "\n",
    "admissionsDf1['AGE'] = (admissionsDf1[\"ADMITTIME\"].values - admissionsDf1[\"DOB\"].values) / np.timedelta64(1,\"D\") // 365 #calculate the age of each patient at the time of admission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a091b60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the patient id for all patients < 15 years old\n",
    "under15 = admissionsDf1.loc[abs(admissionsDf1[\"AGE\"]) < 15.0]\n",
    "under15Patients = list(under15[\"SUBJECT_ID\"]) #7,875 patients under 15 years\n",
    "\n",
    "#note: conflicting information in the paper about age filter (page 1155 indicates >= 15 years while page 1156 indicates\n",
    "# > 15 years). We are using >= 15 years as candidate patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46d18129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop under15 patients from admissions, diagnoses, events and patients\n",
    "admissionsFiltered = admissionsDf1[~admissionsDf1.SUBJECT_ID.isin(under15Patients)]\n",
    "diagnosesFiltered = diagnosesDf[~diagnosesDf.SUBJECT_ID.isin(under15Patients)]\n",
    "eventsFiltered = eventsDf[~eventsDf.SUBJECT_ID.isin(under15Patients)]\n",
    "patientsFiltered = patientsDf[~patientsDf.SUBJECT_ID.isin(under15Patients)] #38,645 adult (>=15 years) patients\n",
    "\n",
    "#Note: the FarSight paper incorrectly states there are 7,704 distinct patients (page 1155)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c5cf7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eventsNoError = eventsFiltered.loc[eventsFiltered.ISERROR != 1] #drop events with known errors. i.e. ISERROR = 1\n",
    "eventsNoDuplicate = eventsNoError.drop_duplicates() #drop duplicate events from the filtered dataframe\n",
    "patientsNoErrors = sorted(list(eventsFiltered.SUBJECT_ID.unique())) #create list of patients from filtered events which have no errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "823d6f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select patient id and hospital admission code from admissions dataframe and use to merge left with diagnoses and events\n",
    "patientHadmCode = admissionsFiltered[[\"SUBJECT_ID\", \"HADM_ID\"]]\n",
    "diagnosesFiltered1 = pd.merge(patientHadmCode, diagnosesFiltered, how=\"left\", on=[\"SUBJECT_ID\", \"HADM_ID\"])\n",
    "eventsFiltered1 = pd.merge(patientHadmCode, eventsNoDuplicate, how=\"left\", on=[\"SUBJECT_ID\", \"HADM_ID\"])\n",
    "eventsFiltered1[[\"TEXT\"]] = eventsFiltered1[[\"TEXT\"]].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "073720fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the final dataframes for further analysis\n",
    "admissions = admissionsFiltered[admissionsFiltered.SUBJECT_ID.isin(patientsNoErrors)] #final admissions dataframe to be used for further analysis\n",
    "diagnoses = diagnosesFiltered1[diagnosesFiltered1.SUBJECT_ID.isin(patientsNoErrors)].sort_values(by=\"SUBJECT_ID\") #final diagnoses dataframe to be used for further analysis\n",
    "patients = patientsFiltered[patientsFiltered.SUBJECT_ID.isin(patientsNoErrors)] #final patients dataframe to be used for further analysis\n",
    "events = eventsFiltered1[eventsFiltered1.SUBJECT_ID.isin(patientsNoErrors)].sort_values(by=[\"SUBJECT_ID\", \"CHARTDATE\"]) #final events dataframe to be used for further analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458e72f0",
   "metadata": {},
   "source": [
    "### Data Aggregation (using FarSight approach)\n",
    "- Concatenate the chronological notes for each patient - DONE!\n",
    "- Create a set/list of all ICD-9 codes for each patient - DONE!\n",
    "- Test set vs list for icd-9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e9ed55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenates the chronological notes for each patient\n",
    "events_grouped  = events.groupby(\"HADM_ID\")[\"TEXT\"].apply(lambda x: \" \".join(x)).reset_index()\n",
    "events_grouped[\"HADM_ID\"] = events_grouped[\"HADM_ID\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "820ccdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes the first 3 digits of each icd9 code for easier categorising\n",
    "diagnoses_categorised = diagnoses.copy()\n",
    "diagnoses_categorised[\"ICD9_CODE\"] = diagnoses_categorised[\"ICD9_CODE\"].astype(str).apply(lambda x: x[0:3])\n",
    "\n",
    "#df.isna or df.dropna couldn't find nan so drop by string location\n",
    "diagnoses_categorised = diagnoses_categorised.loc[diagnoses_categorised[\"ICD9_CODE\"] != \"nan\"]\n",
    "\n",
    "#Converts V and E codes to 1000 for easier binning\n",
    "diagnoses_categorised[\"ICD9_CODE\"] = diagnoses_categorised[\"ICD9_CODE\"].apply(lambda x: \"1000\" if (x[0] == \"V\" or x[0] == \"E\") else x).astype(int)\n",
    "\n",
    "#Categorises the icd9 codes using bins\n",
    "icd9_bins = [0,139,239,279,289,319,389,459,519,579,629,677,709,739,759,789,796,799,999,1000]\n",
    "diagnoses_categorised[\"ICD9_GROUPS\"] = pd.cut(diagnoses_categorised[\"ICD9_CODE\"], bins = icd9_bins, right = True, labels = False)\n",
    "\n",
    "#Groups the diagnoses codes into lists\n",
    "diagnoses_grouped = diagnoses_categorised.groupby(\"HADM_ID\")[\"ICD9_GROUPS\"].apply(set).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c622b",
   "metadata": {},
   "source": [
    "### ICD-9 Category key\n",
    "Key: Code Category\n",
    "\n",
    "- 0: 001-139\n",
    "- 1: 140-239\n",
    "- 2: 240-279\n",
    "- 3: 280-289\n",
    "- 4: 290-319\n",
    "- 5: 320-389\n",
    "- 6: 390-459\n",
    "- 7: 460-519\n",
    "- 8: 520-579\n",
    "- 9: 580-629\n",
    "- 10: 630-677\n",
    "- 11: 680-709\n",
    "- 12: 710-739\n",
    "- 13: 740-759\n",
    "- 14: 760-789\n",
    "- 15: 790-796\n",
    "- 16: 797-799\n",
    "- 17: 800-999\n",
    "- 18: V & E codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "65552558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>ICD9_GROUPS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>[**2117-9-11**] 11:12 AM\\n CHEST (PA &amp; LAT)   ...</td>\n",
       "      <td>{2, 5, 6, 8, 9, 11, 18}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>Sinus rhythm\\nProlonged QT interval is nonspec...</td>\n",
       "      <td>{0, 3, 6, 8, 14}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100006</td>\n",
       "      <td>Sinus tachycardia\\nLeft axis deviation - anter...</td>\n",
       "      <td>{1, 2, 4, 7, 14, 18}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100007</td>\n",
       "      <td>Sinus rhythm\\nAtrial premature complex\\nConsid...</td>\n",
       "      <td>{8, 17, 6, 7}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100009</td>\n",
       "      <td>Sinus bradycardia.  Left atrial abnormality.  ...</td>\n",
       "      <td>{2, 3, 6, 17, 18}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID                                               TEXT  \\\n",
       "0   100001  [**2117-9-11**] 11:12 AM\\n CHEST (PA & LAT)   ...   \n",
       "1   100003  Sinus rhythm\\nProlonged QT interval is nonspec...   \n",
       "2   100006  Sinus tachycardia\\nLeft axis deviation - anter...   \n",
       "3   100007  Sinus rhythm\\nAtrial premature complex\\nConsid...   \n",
       "4   100009  Sinus bradycardia.  Left atrial abnormality.  ...   \n",
       "\n",
       "               ICD9_GROUPS  \n",
       "0  {2, 5, 6, 8, 9, 11, 18}  \n",
       "1         {0, 3, 6, 8, 14}  \n",
       "2     {1, 2, 4, 7, 14, 18}  \n",
       "3            {8, 17, 6, 7}  \n",
       "4        {2, 3, 6, 17, 18}  "
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Merge the nursing notes and the ICD9 groups together\n",
    "aggregated_df = events_grouped.merge(diagnoses_grouped,on = \"HADM_ID\")\n",
    "\n",
    "#create a list of sets where each set contains all icd9 codes for a single patient\n",
    "icd9_grouped = list(aggregated_df[\"ICD9_GROUPS\"]) #list of ICD9 codes\n",
    "\n",
    "#create a list of strings where each string is the concatenated nursing texts for a single patient. \n",
    "notes_list_all = list(aggregated_df[\"TEXT\"])\n",
    "aggregated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "dc81a987",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain the list of patients \n",
    "patients = list(events[[\"SUBJECT_ID\" , \"HADM_ID\"]].drop_duplicates().sort_values(by=\"HADM_ID\")[\"SUBJECT_ID\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ecc86e",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "- Remove multiple spaces and special characters - DONE!\n",
    "- Tokenization (using NLTK) - DONE!\n",
    "- Stopword removal from generated tokens (using the NLTK English stopword corpus) - DONE!\n",
    "- Remove punctuation marks except hyphens and slashes - DONE!\n",
    "- Remove references to images - DONE!\n",
    "- Perform character case folding - DONE!\n",
    "- Perform medical concept normalization through disambiguation of abbreviations (into long form) using CARD - FOLLOW UP!\n",
    "- Perform suffix stripping through stemming - DONE!\n",
    "- Convert stripped tokens into their respective base forms by lemmatization - DONE!\n",
    "- Discard tokens appearing in less than 10 nursing notes - DONE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "4759a2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! use a subset of the data from faster testing. comment out this line for final submission\n",
    "subset = 25\n",
    "notes_list = notes_list_all[:subset]\n",
    "icd9_grouped = icd9_grouped[:subset]\n",
    "patients = patients[:subset]\n",
    "#create a list of labels of shape (subset,19). 1 at an index indicates the icd-9 code group corresponding to that index\n",
    "#is present\n",
    "icd9_labels = np.zeros((subset, 19)) \n",
    "for i in range(subset):\n",
    "    icd9_labels[i][np.array(list(icd9_grouped[i]))] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b2110700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to remove multiple spaces, special characters and newlines\n",
    "def clean_notes(note):\n",
    "    a = re.sub(\"[^a-zA-Z0-9-/]+\", \" \", str(note)) #remove special characters\n",
    "    b = re.sub(\"\\n\", \"\", str(a)) #remove newlines\n",
    "    c = re.sub(\" +\", \" \", str(b)) #remove extra spaces\n",
    "    return c\n",
    "\n",
    "#map the clean notes funciton to the notes_list\n",
    "notes_list = list(map(clean_notes, notes_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a6d2d0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the nursing notes text\n",
    "tokens = [] #empty tokens list\n",
    "stops = set(stopwords.words('english')) #English stopwords corpus\n",
    "pStem = PorterStemmer() #instance of stemmer for suffix stripping. Less aggressive stemmer\n",
    "lStem = LancasterStemmer() #instance of stemmer for suffix stripping. Use either this or pStem\n",
    "wLemma = WordNetLemmatizer() #instance of lemmatizer\n",
    "img1=\".jpeg\"; img2=\".jpg\"; img3=\".png\"; img4=\".tiff\"; img5=\".bmp\" #image references for lookup\n",
    "\n",
    "for idx,txt in enumerate(notes_list):\n",
    "    a = nltk.word_tokenize(txt) #tokenize each nursing note\n",
    "    tokens.append([wLemma.lemmatize(pStem.stem(word.casefold()), pos=\"v\") for word in a if not word in stops \\\n",
    "                   if not img1 in word if not img2 in word if not img3 in word if not img4 in word if not img5 in word])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c11d4de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eliminate tokens appearing in less than 10 nursing texts \n",
    "#NOTE: NEED TO OPTIMIZE!!!\n",
    "token_counts = Counter([token for l in tokens for token in l])\n",
    "under_10_tokens = [t for (t,c) in token_counts.items() if c < 10]\n",
    "        \n",
    "for idx, note in enumerate(tokens):\n",
    "    tokens[idx] = [token for token in note if not token in under_10_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185e9bf4",
   "metadata": {},
   "source": [
    "### NOTE!!!\n",
    "At this point we have:\n",
    "- patients: which is a list of patients ids (age >= 15) arranged in order of ascending HADM_IDs\n",
    "- icd9_grouped: which is a list of sets of icd9 codes where each set contains the icd9 codes for an individual patient. Each set of icd9 codes is for the patient at the corresponding index in patients. \n",
    "- tokens: which is a list of of lists where each list contains the normalized tokens for an individual patient where these tokens are generated using the concatenated nursing notes for said patient. Each list of tokens is for the patient at the corresponding index in patients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e71656a",
   "metadata": {},
   "source": [
    "### Clinical feature modeling - VECTOR SPACE MODELING OF CLINICAL NOTES\n",
    "- Obtain the Doc2Vec style features from the normalized nursing note tokens. Utilize the implementation in the Python Gensim package, with an embedding size of 500 (trained for 25 epochs), determined empirically using grid-search as per the original Farsight paper. - DONE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c4c2c8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of TaggedDocument and train Doc2Vec model \n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(tokens)] #create list of TaggedDocument\n",
    "d2vModel = Doc2Vec(vector_size=500, epochs=25, dm=1) #create model\n",
    "d2vModel.build_vocab(documents) #build the vocab\n",
    "d2vModel.train(documents, total_examples=d2vModel.corpus_count, epochs=d2vModel.epochs) #train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "23cb3218",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#map the tokens to Doc2Vec style features using the trained model\n",
    "d2v_tokens = [d2vModel.infer_vector(i) for i in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b973ce8",
   "metadata": {},
   "source": [
    "### Clinical feature modeling - TOPIC MODELING OF CLINICAL NOTES\n",
    "- Build NMF matrices on BoW (Bag of Words) and TW (Term Weighting) matrices. Model the BoW and TW matrices using NMF both with and without semantic coherence scoring considered (set to 100 topics with SC considered and 150 topics without SC considered). Implement NMF models in the Python Gensim package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "cae87ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a corpus (BoW) from the list of tokens\n",
    "token_dictionary = Dictionary(tokens)\n",
    "token_corpus = [token_dictionary.doc2bow(text) for text in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "435809fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NMF Model #1\n",
    "#train the model on the BoW corpus with SC consideration (num_topics = 100)\n",
    "nmfBoWSCModel = Nmf(token_corpus, num_topics=100)\n",
    "\n",
    "#infer the vector (topic probability distribution) for BoW corpus with SC consideration (num_topics = 100)\n",
    "nmfBoWSC_tokens = nmfBoWSCModel[token_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "2c93f73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NMF Model #2\n",
    "#train the model on the BoW corpus without SC consideration (num_topics = 150)\n",
    "nmfBoWwoSCModel = Nmf(token_corpus, num_topics=150)\n",
    "\n",
    "#infer the vector (topic probability distribution) for BoW corpus with SC consideration (num_topics = 100)\n",
    "nmfBoWwoSC_tokens = nmfBoWwoSCModel[token_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "063495c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply TW (Term Weighting) transformation to the BoW corpus\n",
    "tw_transformer = TfidfModel(token_corpus)\n",
    "tw_corpus = tw_transformer[token_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "0e900e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NMF Model #3\n",
    "#train the model on the TW corpus with SC consideration (num_topics = 100)\n",
    "nmfTWSCModel = Nmf(tw_corpus, num_topics=100)\n",
    "\n",
    "#infer the vector (topic probability distribution) for TW corpus with SC consideration (num_topics = 100)\n",
    "nmfTWSC_tokens = nmfTWSCModel[tw_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "f9165350",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NMF Model #4\n",
    "#train the model on the TW corpus without SC consideration (num_topics = 150)\n",
    "nmfTWwoSCModel = Nmf(tw_corpus, num_topics=150)\n",
    "\n",
    "#infer the vector (topic probability distribution) for BoW corpus without SC consideration (num_topics = 150)\n",
    "nmfTWwoSC_tokens = nmfTWwoSCModel[tw_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2f2fd6",
   "metadata": {},
   "source": [
    "### Deep Neural Architectures\n",
    "Six(6) neural arcitectures (from Python Keras package with Tensorflow backend) employed for the purpose of predicting icd-9 code groups:\n",
    "- MLP\n",
    "- ConvNet\n",
    "- LSTM\n",
    "- Bi-LSTM\n",
    "- Conv-LSTM\n",
    "- Seg-GRU\n",
    "\n",
    "The deep neural models were trained to minimize a cross-entropy loss (mean squared error prediction loss) function using an Adam optimizer, with a batch size of 128, for eight epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b1c82a",
   "metadata": {},
   "source": [
    "#### Neural Architecture #1: MLP \n",
    "MLP with:\n",
    "- one hidden layer of 75 ReLU processing units \n",
    "- and a fully connected prediction layer of 19 sigmoid processing units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "062a95e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results - Loss: 0.5727770328521729 - Accuracy: 0.75%\n"
     ]
    }
   ],
   "source": [
    "#MLP trained on Doc2Vec style features with FarSight aggregation\n",
    "\n",
    "#configuration options \n",
    "feature_vec_len = len(d2v_tokens[0])\n",
    "\n",
    "#split data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.array(d2v_tokens), icd9_labels, test_size=0.30, random_state=123)\n",
    "x_train = x_train.reshape(x_train.shape[0], feature_vec_len)\n",
    "x_test = x_test.reshape(x_test.shape[0], feature_vec_len)\n",
    "\n",
    "#create the model\n",
    "mlpMdl = Sequential()\n",
    "mlpMdl.add(Dense(75, input_shape=x_train[0].shape, activation='relu'))\n",
    "mlpMdl.add(Dense(19, activation='sigmoid'))\n",
    "\n",
    "#configure and train the model\n",
    "mlpMdl.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "mlpMdl.fit(x_train, y_train, epochs=4, batch_size=1, verbose=0, validation_split=0.2)\n",
    "\n",
    "#test the model\n",
    "test_results = mlpMdl.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848f85e9",
   "metadata": {},
   "source": [
    "#### Neural Architecture #2: ConvNet\n",
    "ConvNet with:\n",
    "- one fully connected layer of 289 ReLU processing units\n",
    "- one ConvNet layer with 3x3 convolution window and feature map size of 19\n",
    "- and a fully connected prediction layer of 19 sigmoid processing units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4022e04d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#ConvNet trained on Doc2Vec style features with FarSight aggregation\n",
    "\n",
    "#configuration options \n",
    "feature_vec_len = len(d2v_tokens[0])\n",
    "\n",
    "#split data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.array(d2v_tokens), icd9_labels, test_size=0.30, random_state=123)\n",
    "x_train = x_train.reshape(x_train.shape[0], feature_vec_len)\n",
    "x_test = x_test.reshape(x_test.shape[0], feature_vec_len)\n",
    "\n",
    "#create the model\n",
    "convNetMdl = Sequential()\n",
    "convNetMdl.add(Dense(289, input_shape=x_train[0].shape, activation='relu'))\n",
    "convNetMdl.add(LSTM(19, kernel_size=3))\n",
    "convNetMdl.add(Dense(19, activation='sigmoid'))\n",
    "\n",
    "#configure and train the model\n",
    "convNetMdl.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "convNetMdl.fit(x_train, y_train, epochs=4, batch_size=1, verbose=0, validation_split=0.2)\n",
    "\n",
    "#test the model\n",
    "test_results = convNetMdl.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a281dc60",
   "metadata": {},
   "source": [
    "#### Neural Architecture #3: LSTM \n",
    "LSTM with:\n",
    "- one fully connected layer of 289 ReLU processing units,  \n",
    "- and an LSTM layer with 300 hidden nodes. \n",
    "- ICD-9 code group prediction is facilitated by a sigmoid activation of the final LSTM output on a fully connected layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c451ecb3",
   "metadata": {},
   "source": [
    "#LSTM trained on Doc2Vec style features with FarSight aggregation\n",
    "\n",
    "#configuration options \n",
    "feature_vec_len = len(d2v_tokens[0])\n",
    "\n",
    "#split data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.array(d2v_tokens), icd9_labels, test_size=0.30, random_state=123)\n",
    "x_train = x_train.reshape(x_train.shape[0], feature_vec_len)\n",
    "x_test = x_test.reshape(x_test.shape[0], feature_vec_len)\n",
    "\n",
    "#create the model\n",
    "lstmMdl = Sequential()\n",
    "lstmMdl.add(Dense(289, input_shape=x_train[0].shape, activation='relu', ))\n",
    "lstmMdl.add(LSTM(300, input_shape=(289, 1), return_sequences=True))\n",
    "lstmMdl.add(Dense(19, activation='sigmoid'))\n",
    "\n",
    "#configure and train the model\n",
    "lstmMdl.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "lstmMdl.fit(x_train, y_train, epochs=4, batch_size=1, verbose=0, validation_split=0.2)\n",
    "\n",
    "#test the model\n",
    "test_results = lstmMdl.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]}%')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
